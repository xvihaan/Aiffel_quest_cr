{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e5c10c9",
   "metadata": {},
   "source": [
    "# 👻 [프로젝트] 커스텀 프로젝트 직접 만들기\n",
    "KLUE/BERT-base 모델 활용, NSMC Task\n",
    "\n",
    "## STEP 1. NSMC 데이터 분석 및 Huggingface dataset 구성\n",
    "## STEP 2. klue/bert-base model 및 tokenizer 불러오기\n",
    "## STEP 3. 위에서 불러온 tokenizer으로 데이터셋을 전처리하고, model 학습 진행해 보기\n",
    "## STEP 4. Fine-tuning을 통하여 모델 성능(accuarcy) 향상시키기\n",
    "## STEP 5. Bucketing을 적용하여 학습시키고, STEP 4의 결과와의 비교\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e32f836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n",
      "1.21.4\n",
      "4.11.3\n",
      "1.14.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow\n",
    "import numpy\n",
    "import transformers\n",
    "import datasets\n",
    "\n",
    "print(tensorflow.__version__)\n",
    "print(numpy.__version__)\n",
    "print(transformers.__version__)\n",
    "print(datasets.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2015927",
   "metadata": {},
   "source": [
    "## STEP 1. NSMC 데이터 분석 및 Huggingface dataset 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26a8aa93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset nsmc (/aiffel/.cache/huggingface/datasets/nsmc/default/1.1.0/bfd4729bf1a67114e5267e6916b9e4807010aeb238e4a3c2b95fbfa3a014b5f3)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3958165fa91f4170ba3424a0c594d4d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'document', 'label'],\n",
      "        num_rows: 150000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'document', 'label'],\n",
      "        num_rows: 50000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Huggingface dataset에서 NSMC 데이터셋 불러오기\n",
    "from datasets import load_dataset\n",
    "\n",
    "huggingface_nsmc_dataset = load_dataset('nsmc')\n",
    "print(huggingface_nsmc_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "825f1209",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id', 'document', 'label']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train datasets의 각 컬럼에 해당하는 요소\n",
    "train = huggingface_nsmc_dataset['train']\n",
    "cols = train.column_names\n",
    "cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7c49f8",
   "metadata": {},
   "source": [
    "### 비상비상 🚨🚨🚨\n",
    "\n",
    "데이터가 너무 많아서 훈련할 때 10시간이나 기다렸어야 해서 데이터를 줄이는 방식을 택했다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b4a02a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset nsmc (/aiffel/.cache/huggingface/datasets/nsmc/default/1.1.0/bfd4729bf1a67114e5267e6916b9e4807010aeb238e4a3c2b95fbfa3a014b5f3)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a71d75b8e6d4225847dee1a3cc787ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached split indices for dataset at /aiffel/.cache/huggingface/datasets/nsmc/default/1.1.0/bfd4729bf1a67114e5267e6916b9e4807010aeb238e4a3c2b95fbfa3a014b5f3/cache-9d879241da83f708.arrow and /aiffel/.cache/huggingface/datasets/nsmc/default/1.1.0/bfd4729bf1a67114e5267e6916b9e4807010aeb238e4a3c2b95fbfa3a014b5f3/cache-0fdc790e2ba4f643.arrow\n",
      "Loading cached split indices for dataset at /aiffel/.cache/huggingface/datasets/nsmc/default/1.1.0/bfd4729bf1a67114e5267e6916b9e4807010aeb238e4a3c2b95fbfa3a014b5f3/cache-5dcc18b03997bc21.arrow and /aiffel/.cache/huggingface/datasets/nsmc/default/1.1.0/bfd4729bf1a67114e5267e6916b9e4807010aeb238e4a3c2b95fbfa3a014b5f3/cache-d0620966fea985b9.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['id', 'document', 'label'],\n",
      "    num_rows: 15000\n",
      "})\n",
      "Dataset({\n",
      "    features: ['id', 'document', 'label'],\n",
      "    num_rows: 5000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# 데이터셋 불러오기\n",
    "huggingface_nsmc_dataset = load_dataset('nsmc')\n",
    "\n",
    "# 데이터셋 크기를 줄이기 위해 10%만 샘플링\n",
    "hf_train_dataset = huggingface_nsmc_dataset['train'].train_test_split(test_size=0.1, seed=42)['test']\n",
    "hf_test_dataset = huggingface_nsmc_dataset['test'].train_test_split(test_size=0.1, seed=42)['test']\n",
    "\n",
    "# 샘플링된 데이터셋 확인\n",
    "print(hf_train_dataset)\n",
    "print(hf_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db8aef73",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id : 9976970\n",
      "document : 아 더빙.. 진짜 짜증나네요 목소리\n",
      "label : 0\n",
      "\n",
      "\n",
      "id : 3819312\n",
      "document : 흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나\n",
      "label : 1\n",
      "\n",
      "\n",
      "id : 10265843\n",
      "document : 너무재밓었다그래서보는것을추천한다\n",
      "label : 0\n",
      "\n",
      "\n",
      "id : 9045019\n",
      "document : 교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정\n",
      "label : 0\n",
      "\n",
      "\n",
      "id : 6483659\n",
      "document : 사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 던스트가 너무나도 이뻐보였다\n",
      "label : 1\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    for col in cols:\n",
    "        print(col, \":\", train[col][i])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e964b1e",
   "metadata": {},
   "source": [
    "## STEP 2. klue/bert-base model 및 tokenizer 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b396b8db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# klue/bert-base 모델과 토크나이저 불러오기\n",
    "huggingface_tokenizer = AutoTokenizer.from_pretrained('klue/bert-base')\n",
    "huggingface_model = AutoModelForSequenceClassification.from_pretrained('klue/bert-base', num_labels=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b644c1",
   "metadata": {},
   "source": [
    "Huggingface에서는 AutoTokenizer와 AutoModel 기능을 제공하여, pretrained 모델의 경로 또는 이름만 알면 자동으로 모델을 생성할 수 있습니다.\n",
    "\n",
    "예를 들어, BERT와 RoBERTa 모델을 사용할 때 각각의 토크나이저를 자동으로 선택할 수 있습니다. 특정 작업에 맞는 모델을 사용하기 위해 AutoModelForSequenceClassification을 권장하며, 다양한 모델을 실험할 수 있는 장점이 있습니다.\n",
    "\n",
    "토크나이징은 transform 함수를 사용하여 데이터셋의 형태에 맞춰 진행하며, 문장이 길 경우 truncation을 통해 짧게 자를 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c455202",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(data):\n",
    "    return huggingface_tokenizer(\n",
    "        data['document'],\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        return_token_type_ids=False,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed29a68b",
   "metadata": {},
   "source": [
    "이렇게 하면 document 열을 기반으로 토큰화가 진행.\n",
    "\n",
    "데이터셋을 한번에 토크나이징할때 자주 사용하는 기법은 map입니다.\n",
    "\n",
    "map을 사용하게 되면 Data dictionary에 있는 모든 데이터들이 빠르게 적용시킬 수 있습니다.\n",
    "\n",
    "우리는 map을 사용해 토크나이징을 진행하기 때문에 batch를 적용해야 되므로 batched=True로 주어야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "029dca9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "503f5a33da0241e98de5be1410be8c54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48e08ffcace44af5a9faffe0e0215747",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['attention_mask', 'document', 'id', 'input_ids', 'label'],\n",
      "    num_rows: 15000\n",
      "})\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'document', 'id', 'input_ids', 'label'],\n",
      "    num_rows: 5000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 샘플링된 데이터셋에 transform 함수 적용\n",
    "hf_train_dataset = hf_train_dataset.map(transform, batched=True)\n",
    "hf_test_dataset = hf_test_dataset.map(transform, batched=True)\n",
    "\n",
    "# 적용된 데이터셋 확인\n",
    "print(hf_train_dataset)\n",
    "print(hf_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acdf8e45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6250083",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fae6965",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e0f792d7",
   "metadata": {},
   "source": [
    "## STEP 3. 위에서 불러온 tokenizer으로 데이터셋을 전처리하고, model 학습 진행해 보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b977013",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "output_dir = os.getenv('HOME')+'/aiffel/transformers'\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir,                                         # output이 저장될 경로\n",
    "    evaluation_strategy=\"epoch\",           #evaluation하는 빈도\n",
    "    learning_rate = 2e-5,                         #learning_rate\n",
    "    per_device_train_batch_size = 8,   # 각 device 당 batch size\n",
    "    per_device_eval_batch_size = 8,    # evaluation 시에 batch size\n",
    "    num_train_epochs = 3,                     # train 시킬 총 epochs\n",
    "    weight_decay = 0.01,                        # weight decay\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f59bd45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "485cd2adb17b4b7e82485bb26fcc1310",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a129c015758242f2bd36b5eed3fdeffd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 15000\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 5625\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5625' max='5625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5625/5625 1:21:42, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.340800</td>\n",
       "      <td>0.311782</td>\n",
       "      <td>0.874800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.226400</td>\n",
       "      <td>0.434459</td>\n",
       "      <td>0.877400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.109700</td>\n",
       "      <td>0.596637</td>\n",
       "      <td>0.878800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-500\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-1000\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-1000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-1000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-1500\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-1500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-1500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-2000\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-2000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-2000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-2500\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-2500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-2500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-3000\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-3000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-3000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-3500\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-3500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-3500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-4000\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-4000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-4000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-4500\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-4500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-4500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-5500\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-5500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-5500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=5625, training_loss=0.2486114040798611, metrics={'train_runtime': 4903.6732, 'train_samples_per_second': 9.177, 'train_steps_per_second': 1.147, 'total_flos': 1.18399974912e+16, 'train_loss': 0.2486114040798611, 'epoch': 3.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from transformers import Trainer, TrainingArguments, AutoTokenizer, AutoModelForSequenceClassification\n",
    "from datasets import load_metric, load_dataset\n",
    "\n",
    "# 전체 데이터셋에 토크나이징 적용\n",
    "hf_train_dataset = hf_train_dataset.map(transform, batched=True)\n",
    "hf_test_dataset = hf_test_dataset.map(transform, batched=True)\n",
    "\n",
    "# 필요 없는 열 제거\n",
    "hf_train_dataset = hf_train_dataset.remove_columns(['id', 'document'])\n",
    "hf_test_dataset = hf_test_dataset.remove_columns(['id', 'document'])\n",
    "\n",
    "# 분류를 위한 accuracy metric 설정\n",
    "metric = load_metric('accuracy')\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "# Trainer 설정\n",
    "trainer = Trainer(\n",
    "    model=huggingface_model,                          # 학습할 모델\n",
    "    args=training_arguments,              # 설정된 TrainingArguments\n",
    "    train_dataset=hf_train_dataset,       # 학습 데이터셋\n",
    "    eval_dataset=hf_test_dataset,         # 평가 데이터셋\n",
    "    compute_metrics=compute_metrics       # 성능 평가 함수\n",
    ")\n",
    "\n",
    "# 학습 수행\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a46e03a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='625' max='625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [625/625 02:50]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.5966372489929199,\n",
       " 'eval_accuracy': 0.8788,\n",
       " 'eval_runtime': 170.9164,\n",
       " 'eval_samples_per_second': 29.254,\n",
       " 'eval_steps_per_second': 3.657,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(hf_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a99801ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f52c082",
   "metadata": {},
   "source": [
    "## STEP 4. Fine-tuning을 통하여 모델 성능(accuarcy) 향상시키기\n",
    "- 데이터 전처리, TrainingArguments 등을 조정하여 모델의 정확도를 90% 이상으로 끌어올려봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d2771e4b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using amp fp16 backend\n",
      "***** Running training *****\n",
      "  Num examples = 15000\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 8\n",
      "  Total optimization steps = 4685\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4685' max='4685' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4685/4685 1:17:11, Epoch 4/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.136200</td>\n",
       "      <td>0.527048</td>\n",
       "      <td>0.870200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.122700</td>\n",
       "      <td>0.635770</td>\n",
       "      <td>0.876600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.047600</td>\n",
       "      <td>0.817771</td>\n",
       "      <td>0.879800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.034400</td>\n",
       "      <td>0.887424</td>\n",
       "      <td>0.879400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.027200</td>\n",
       "      <td>0.925307</td>\n",
       "      <td>0.878400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/transformers/trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-937\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-937/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-937/pytorch_model.bin\n",
      "Deleting older checkpoint [/aiffel/aiffel/transformers/checkpoint-500] due to args.save_total_limit\n",
      "Deleting older checkpoint [/aiffel/aiffel/transformers/checkpoint-1000] due to args.save_total_limit\n",
      "Deleting older checkpoint [/aiffel/aiffel/transformers/checkpoint-1500] due to args.save_total_limit\n",
      "Deleting older checkpoint [/aiffel/aiffel/transformers/checkpoint-2000] due to args.save_total_limit\n",
      "Deleting older checkpoint [/aiffel/aiffel/transformers/checkpoint-2500] due to args.save_total_limit\n",
      "Deleting older checkpoint [/aiffel/aiffel/transformers/checkpoint-3000] due to args.save_total_limit\n",
      "Deleting older checkpoint [/aiffel/aiffel/transformers/checkpoint-3500] due to args.save_total_limit\n",
      "Deleting older checkpoint [/aiffel/aiffel/transformers/checkpoint-4000] due to args.save_total_limit\n",
      "Deleting older checkpoint [/aiffel/aiffel/transformers/checkpoint-4500] due to args.save_total_limit\n",
      "Deleting older checkpoint [/aiffel/aiffel/transformers/checkpoint-5000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-1874\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-1874/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-1874/pytorch_model.bin\n",
      "Deleting older checkpoint [/aiffel/aiffel/transformers/checkpoint-5500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-2811\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-2811/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-2811/pytorch_model.bin\n",
      "Deleting older checkpoint [/aiffel/aiffel/transformers/checkpoint-1874] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-3748\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-3748/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-3748/pytorch_model.bin\n",
      "Deleting older checkpoint [/aiffel/aiffel/transformers/checkpoint-2811] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-4685\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-4685/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-4685/pytorch_model.bin\n",
      "Deleting older checkpoint [/aiffel/aiffel/transformers/checkpoint-3748] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /aiffel/aiffel/transformers/checkpoint-937 (score: 0.5270480513572693).\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5000' max='2500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2500/2500 06:55]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "평가 결과: {'eval_loss': 0.5270480513572693, 'eval_accuracy': 0.8702, 'eval_runtime': 190.5614, 'eval_samples_per_second': 26.238, 'eval_steps_per_second': 13.119, 'epoch': 5.0}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# 저장 경로 설정\n",
    "output_dir = os.getenv('HOME')+'/aiffel/transformers'\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,                       # 모델이 저장될 경로\n",
    "    evaluation_strategy=\"epoch\",                 # 매 에포크마다 평가\n",
    "    save_strategy=\"epoch\",                       # 매 에포크마다 모델 저장\n",
    "    learning_rate=2e-5,                          # 낮은 학습률을 사용해 세밀하게 조정\n",
    "    per_device_train_batch_size=2,    A          # 학습 시 배치 크기를 16으로 설정\n",
    "    per_device_eval_batch_size=2,               # 평가 시 배치 크기를 16으로 설정\n",
    "    num_train_epochs=5,                          # 총 학습 에포크 수\n",
    "    weight_decay=0.01,                           # 가중치 감소 설정\n",
    "    logging_dir='./logs',                        # 로깅을 위한 디렉토리\n",
    "    logging_steps=50,                            # 로그를 찍을 스텝 수\n",
    "    save_total_limit=2,                          # 저장되는 체크포인트 개수 제한\n",
    "    load_best_model_at_end=True,                 # 최고의 모델만을 저장\n",
    "    gradient_accumulation_steps=8,   # 가상 배치 크기 증가\n",
    "    fp16=True,   # 혼합 정밀도 사용\n",
    ")\n",
    "\n",
    "# Trainer 설정은 이전과 동일합니다.\n",
    "trainer = Trainer(\n",
    "    model=huggingface_model,                     # 학습할 모델\n",
    "    args=training_arguments,                     # 설정한 TrainingArguments\n",
    "    train_dataset=hf_train_dataset,           # 학습 데이터셋\n",
    "    eval_dataset=hf_test_dataset,             # 평가 데이터셋\n",
    "    compute_metrics=compute_metrics,             # 정확도 및 기타 평가 지표\n",
    ")\n",
    "\n",
    "# 모델 학습\n",
    "trainer.train()\n",
    "\n",
    "# 평가\n",
    "eval_results = trainer.evaluate()\n",
    "print(\"평가 결과:\", eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ec64d822",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.5270480513572693,\n",
       " 'eval_accuracy': 0.8702,\n",
       " 'eval_runtime': 190.6608,\n",
       " 'eval_samples_per_second': 26.225,\n",
       " 'eval_steps_per_second': 13.112,\n",
       " 'epoch': 5.0}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(hf_test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1753f807",
   "metadata": {},
   "source": [
    "fine-tuning을 했는데 accuracy가 조오금 더 떨어졌다!ㅎ.. 아무래도 데이터를 너무 줄어서 그런가보다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780a62b3",
   "metadata": {},
   "source": [
    "### 오류 !! \n",
    "런타임 오류: CUDA 메모리가 부족합니다. 20.00 MiB를 할당하려고 시도했습니다(GPU 0, 총 용량 14.58 GiB, 이미 할당된 13.28 GiB, 사용 가능한 1.56 MiB, PyTorch에서 총 13.37 GiB 예약).\n",
    "\n",
    "가용할 수 있는 memory가 부족해서 생긴 오류인 것 같다 ㅜㅜ 남아있는 공간이 어느정도 인지 확인해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1245a6a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.9/site-packages (5.8.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install psutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "10d1955a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total RAM: 17.57 GB\n",
      "Available RAM: 12.19 GB\n",
      "Used RAM: 4.98 GB\n",
      "RAM Usage: 30.6%\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "\n",
    "def get_ram_info():\n",
    "    ram_info = psutil.virtual_memory()\n",
    "    total_ram = ram_info.total / (1024 ** 3)  # GB 단위\n",
    "    available_ram = ram_info.available / (1024 ** 3)  # GB 단위\n",
    "    used_ram = ram_info.used / (1024 ** 3)  # GB 단위\n",
    "    ram_usage_percent = ram_info.percent\n",
    "    \n",
    "    print(f\"Total RAM: {total_ram:.2f} GB\")\n",
    "    print(f\"Available RAM: {available_ram:.2f} GB\")\n",
    "    print(f\"Used RAM: {used_ram:.2f} GB\")\n",
    "    print(f\"RAM Usage: {ram_usage_percent}%\")\n",
    "\n",
    "get_ram_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e669b697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Memory: 17.57 GiB\n",
      "Available Memory: 12.17 GiB\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "\n",
    "# 메모리 정보를 GiB 단위로 가져오기\n",
    "total_memory = psutil.virtual_memory().total / (1024 ** 3)\n",
    "available_memory = psutil.virtual_memory().available / (1024 ** 3)\n",
    "\n",
    "print(f\"Total Memory: {total_memory:.2f} GiB\")\n",
    "print(f\"Available Memory: {available_memory:.2f} GiB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5b1a08",
   "metadata": {},
   "source": [
    "메모리 확인 결과 사용가능한 메모리가 살짝 모자라서 torch.cuda.empty_cache()를 선언해서 해결했다.\n",
    "또한, batch_size도 8에서 2로 낮춰서 최대로 낮게 돌려봤다.."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b50014",
   "metadata": {},
   "source": [
    "## STEP 5. Bucketing을 적용하여 학습시키고, STEP 4의 결과와의 비교\n",
    "- STEP 4에 학습한 결과와 bucketing을 적용하여 학습시킨 결과를 비교해보고, 모델 성능 향상과 훈련 시간 두 가지 측면에서 각각 어떤 이점이 있는지 비교"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81b2b01",
   "metadata": {},
   "source": [
    "1. Bucketing과 Dynamic Padding이란?\n",
    "- Bucketing: 데이터를 유사한 길이별로 그룹화하여 배치를 구성하는 방식입니다. 모델이 패딩 토큰을 덜 처리하도록 하여 계산량을 줄입니다.\n",
    "- Dynamic Padding: 배치마다 패딩을 동적으로 설정하여, 각 배치 내에서 가장 긴 문장의 길이만큼만 패딩을 적용해 불필요한 연산을 줄입니다.\n",
    "\n",
    "2. group_by_length와 DataCollatorWithPadding 사용\n",
    "- Trainer에서 bucketing과 dynamic padding을 구현하려면 group_by_length=True와 DataCollatorWithPadding를 설정하면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "016cb88f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running training *****\n",
      "  Num examples = 15000\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 5625\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5625' max='5625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5625/5625 1:20:54, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.032600</td>\n",
       "      <td>0.903095</td>\n",
       "      <td>0.868200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.058800</td>\n",
       "      <td>0.870570</td>\n",
       "      <td>0.871200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.022900</td>\n",
       "      <td>1.007563</td>\n",
       "      <td>0.874000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-1875\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-1875/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-1875/pytorch_model.bin\n",
      "tokenizer config file saved in /aiffel/aiffel/transformers/checkpoint-1875/tokenizer_config.json\n",
      "Special tokens file saved in /aiffel/aiffel/transformers/checkpoint-1875/special_tokens_map.json\n",
      "Deleting older checkpoint [/aiffel/aiffel/transformers/checkpoint-937] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-3750\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-3750/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-3750/pytorch_model.bin\n",
      "tokenizer config file saved in /aiffel/aiffel/transformers/checkpoint-3750/tokenizer_config.json\n",
      "Special tokens file saved in /aiffel/aiffel/transformers/checkpoint-3750/special_tokens_map.json\n",
      "Deleting older checkpoint [/aiffel/aiffel/transformers/checkpoint-4685] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-5625\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-5625/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-5625/pytorch_model.bin\n",
      "tokenizer config file saved in /aiffel/aiffel/transformers/checkpoint-5625/tokenizer_config.json\n",
      "Special tokens file saved in /aiffel/aiffel/transformers/checkpoint-5625/special_tokens_map.json\n",
      "Deleting older checkpoint [/aiffel/aiffel/transformers/checkpoint-1875] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /aiffel/aiffel/transformers/checkpoint-3750 (score: 0.8705695271492004).\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='625' max='625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [625/625 02:56]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucketing 및 Dynamic Padding 적용 후 평가 결과: {'eval_loss': 0.8705695271492004, 'eval_accuracy': 0.8712, 'eval_runtime': 176.8874, 'eval_samples_per_second': 28.267, 'eval_steps_per_second': 3.533, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "from datasets import load_metric\n",
    "\n",
    "# 데이터 Collator - dynamic padding을 적용\n",
    "data_collator = DataCollatorWithPadding(tokenizer=huggingface_tokenizer)\n",
    "\n",
    "# 모델 평가를 위한 metric 설정\n",
    "metric = load_metric(\"accuracy\")  # 기존의 `compute_metrics` 함수로 정확도를 계산하는 metric입니다.\n",
    "\n",
    "# Fine-tuning을 위한 TrainingArguments 설정 (Bucketing 및 Dynamic Padding 적용)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,  # 필요 시 batch 크기 조정\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=50,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    group_by_length=True  # 길이별로 배치 구성 (Bucketing)\n",
    ")\n",
    "\n",
    "# Trainer 설정\n",
    "trainer_with_bucketing = Trainer(\n",
    "    model=huggingface_model,\n",
    "    args=training_args,\n",
    "    train_dataset=hf_train_dataset,\n",
    "    eval_dataset=hf_test_dataset,\n",
    "    tokenizer=huggingface_tokenizer,  # dynamic padding을 위한 tokenizer 전달\n",
    "    data_collator=data_collator,      # dynamic padding을 적용하는 collator\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# 모델 학습\n",
    "trainer_with_bucketing.train()\n",
    "\n",
    "# 평가\n",
    "eval_results_bucketing = trainer_with_bucketing.evaluate()\n",
    "print(\"Bucketing 및 Dynamic Padding 적용 후 평가 결과:\", eval_results_bucketing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de8c4aa",
   "metadata": {},
   "source": [
    "### 첫 훈련, 파인튜닝, Bucketing 결과 정리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ec6229",
   "metadata": {},
   "source": [
    "| |First Learning|Fine-tuning|Bucketing|\n",
    "|---|---|---|---|\n",
    "|시간|4903.6732s(81분)|4680s(78분)|4854s(80분)|\n",
    "|acuuracy|0.8788|0.8702|0.8712|\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46287d12",
   "metadata": {},
   "source": [
    "동일한 조건은 원래 데이터의 10%만 남긴 train data 15000개, test data 5000개로 돌렸다.\n",
    "\n",
    "첫 훈련과 파인튜닝, Bucketing 을 시도했을 때, 다이나믹하게 성능이 바뀌지는 않았다.\n",
    "\n",
    "물론, 데이터의 개수가 너무 작게잡아서 그런 것 같지만. 시간이 없는 관계로 더 시도 해보지는 못하였다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40776c32",
   "metadata": {},
   "source": [
    "## 회고\n",
    "🤗Hugging face로 모델을 받아와서 사용해보는 경험은 사실 경험이 많았다.\n",
    "\n",
    "하지만, 쉽게 사용할 수 있다는 장점으로 허깅페이스에 대해서는 깊게 알고 있지는 않았는데\n",
    "\n",
    "이번 고잉디퍼 학습에서 몰랐던 사실과 커스텀 데이터셋을 이용해 커스텀 모델을 가져와 프로젝트를 진행할 수 있다는 과정은 정말 좋았던 것 같다.\n",
    "\n",
    "좋은 모델을 만들어 나도 내가 지은 커스텀 모델을 배포 해보고 싶다는 생각이 들었다:)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
